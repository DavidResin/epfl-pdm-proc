{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from operator import or_ as union\n",
    "from functools import reduce, partial\n",
    "\n",
    "from bokeh import palettes\n",
    "from bokeh.io import output_file\n",
    "from bokeh.plotting import figure, save, reset_output\n",
    "from bokeh.models import ColumnDataSource, GroupFilter, CDSView, HoverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.getcwd()\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(root_path, \"data\")\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = os.path.join(data_path, \"raw\")\n",
    "raw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_path = os.path.join(data_path, \"parsed\")\n",
    "parsed_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"entries.json\"), \"r\") as f:\n",
    "    mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "    \"beers-law-lab_en-phetio.html?phetioStandalone&phetioLog=console:983 \": \"\",\n",
    "    \"\\u200b\": \"\",\n",
    "    \"[Intervention] Unable to preventDefault inside passive event listener due to target being treated as passive. See <URL>\": \"\"\n",
    "}\n",
    "\n",
    "wanted_rows = [\"phetioID\", \"event\", \"time\", \"parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = dict()\n",
    "\n",
    "entries[\"general\"] = [\n",
    "    'ruler',\n",
    "    'concentration',\n",
    "    'wavelength',\n",
    "    'container',\n",
    "    'solution',\n",
    "    'window',\n",
    "    'probe',\n",
    "    'sim',\n",
    "]\n",
    "\n",
    "entries[\"drags\"] = [\n",
    "    'ruler',\n",
    "    'concentration',\n",
    "    'wavelength',\n",
    "    'container',\n",
    "    'probe',\n",
    "]\n",
    "\n",
    "entries[\"laser\"] = [\n",
    "    'laser',\n",
    "]\n",
    "\n",
    "entries[\"pdf\"] = [\n",
    "    'pdf',\n",
    "]\n",
    "\n",
    "entries[\"modes\"] = [\n",
    "    'transmittance',\n",
    "    'absorbance',\n",
    "]\n",
    "\n",
    "entries[\"other\"] = [\n",
    "    'concentration',\n",
    "    'wavelength',\n",
    "    'solution',\n",
    "    'window',\n",
    "    'sim',\n",
    "]\n",
    "\n",
    "palette = dict()\n",
    "\n",
    "palette[\"general\"] = dict(zip(entries[\"general\"] + entries[\"modes\"], palettes.Category10[10]))\n",
    "palette[\"toggles\"] = dict(zip(entries[\"pdf\"] + entries[\"laser\"],     palettes.Colorblind[8][6:]))\n",
    "\n",
    "figure_kwargs = {\n",
    "    'plot_width': 1200,\n",
    "    'x_axis_label': 'Time (seconds)',\n",
    "    'y_axis_label': 'UIDs',\n",
    "    'toolbar_location': \"below\",\n",
    "}\n",
    "\n",
    "alpha = 0.7\n",
    "\n",
    "common_kwargs = {\n",
    "    'fill_alpha': alpha,\n",
    "    'line_alpha': min(1, alpha + .2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Quickly replace multiple strings based on a dict\n",
    "def replace_all(string, rep):\n",
    "    for k, v in rep.items():\n",
    "        string = string.replace(k, v)\n",
    "    return string\n",
    "\n",
    "# Separate jsons from a raw log file string\n",
    "def split_jsons(log):\n",
    "    mem, res, fails = \"\", [], []\n",
    "    elems = log.split(\"\\n{\")\n",
    "    elems = [elems[0]] + [\"\\n{\" + elem for elem in elems[1:]]\n",
    "    \n",
    "    for elem in elems:\n",
    "        try:\n",
    "            res.append(json.loads(elem))\n",
    "        except:\n",
    "            fails.append(elem)\n",
    "\n",
    "    return res, fails\n",
    "\n",
    "# Non tail-recursive flatten operation\n",
    "# If keep_kids is True, retain only the head element\n",
    "def flatten(entry, keep_kids=True):\n",
    "    copy = entry.copy()\n",
    "    children = copy.get(\"children\")\n",
    "    ret = [copy]\n",
    "    \n",
    "    if children:\n",
    "        copy.pop(\"children\")\n",
    "        \n",
    "        if keep_kids:\n",
    "            for c in children:\n",
    "                ret += flatten(c)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "# Parse a log file into a list of json elements\n",
    "def parse_log_file(path, replacements, wanted_rows, keep_kids=True):    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    if len(data) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    data = replace_all(data, replacements)\n",
    "    jsons, f = split_jsons(data)\n",
    "    \n",
    "    fails = dict()\n",
    "    fails[\"json\"] = f\n",
    "    fails[\"flatten\"] = []\n",
    "    batch = []\n",
    "    \n",
    "    for elem in jsons:\n",
    "        try:\n",
    "            batch += flatten(elem, keep_kids)\n",
    "        except:\n",
    "            fails[\"flatten\"].append(elem)\n",
    "            \n",
    "    return pd.DataFrame(batch)[wanted_rows], fails\n",
    "\n",
    "# Gather all usernames found in a given path\n",
    "def get_usernames(path):\n",
    "    fns = os.listdir(path)\n",
    "    uns = {fn.split(\"-\")[0] for fn in fns}\n",
    "    return uns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(parsed_path, \"{}.pkl\")\n",
    "mem_f = []\n",
    "run = False\n",
    "\n",
    "if run:\n",
    "    for session in os.listdir(raw_path):\n",
    "        s_path = os.path.join(raw_path, session)\n",
    "\n",
    "        with open(out_path.format(session), \"wb\") as f_out:\n",
    "            for uid in get_usernames(s_path):\n",
    "                u_path = os.path.join(s_path, uid + \"-{}.log\")\n",
    "                curr_data = dict()\n",
    "\n",
    "                for exp_idx in range(3):\n",
    "                    batch, fails = parse_log_file(u_path.format(exp_idx + 1), replacements, wanted_rows, keep_kids=False)\n",
    "                    curr_data[exp_idx] = batch\n",
    "\n",
    "                    if fails and (fails[\"json\"] or fails[\"flatten\"]):\n",
    "                        print(session, \"-\", uid, exp_idx, \":\", len(fails[\"json\"]), \"/\", len(fails[\"flatten\"]))\n",
    "                        mem_f.append((session, uid, exp_idx, fails))\n",
    "\n",
    "                pkl.dump((uid, curr_data), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "\n",
    "# REMOVE THIS\n",
    "ds = []\n",
    "\n",
    "with open(os.path.join(parsed_path, os.listdir(parsed_path)[10]), \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            uid, d = pkl.load(f)\n",
    "            ds.append(d)\n",
    "            #if uid == \"rve4n5nv\":\n",
    "                #break\n",
    "            uids.append(uid)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_previous(df, prev_idx):\n",
    "    prev_a = df.loc[prev_idx, \"action\"]\n",
    "    \n",
    "    # Previous was a drag, make it a drag-end\n",
    "    if prev_a == \"drag\":\n",
    "        df.loc[prev_idx, \"action\"] = \"drag-end\"\n",
    "    # Previous was a drag-start, just wipe it\n",
    "    else:\n",
    "        df.drop(prev_idx, inplace=True)\n",
    "\n",
    "def fix_drags(drags, clear_dummies=True):\n",
    "    obj, prev_idx = None, None\n",
    "    \n",
    "    if not len(drags):\n",
    "        return drags\n",
    "    \n",
    "    for idx, row in drags.iterrows():\n",
    "        n_act = row[\"action\"]\n",
    "        n_obj = row[\"object\"]\n",
    "\n",
    "        if \"start\" in n_act:\n",
    "            # Previous was not ended, need to close it\n",
    "            if obj is not None:\n",
    "                reassign_previous(drags, prev_idx)\n",
    "\n",
    "            # Memorize new object\n",
    "            obj = n_obj\n",
    "\n",
    "        elif \"end\" in n_act:\n",
    "            # No matching start row, just wipe current row\n",
    "            if obj != n_obj:\n",
    "                drags.drop(idx, inplace=True)\n",
    "\n",
    "                # Different previous object, need to close it\n",
    "                if obj is not None:\n",
    "                    reassign_previous(drags, prev_idx)\n",
    "\n",
    "            # Reset memorized object\n",
    "            obj = None\n",
    "\n",
    "        else:\n",
    "            # No matching start row, just make this the start row    \n",
    "            if obj != n_obj:   \n",
    "                drags.loc[idx, \"action\"] = \"drag-start\"\n",
    "\n",
    "                # Different previous object, need to close it\n",
    "                if obj is not None:\n",
    "                    reassign_previous(drags, prev_idx)\n",
    "\n",
    "            # Memorize object in any case\n",
    "            obj = n_obj\n",
    "\n",
    "        # Memorize previous index\n",
    "        prev_idx = idx\n",
    "\n",
    "    # Fix last entry if not an end\n",
    "    drags.loc[prev_idx, \"action\"] = \"drag-end\"\n",
    "    \n",
    "    # Remove dummy entries if asked\n",
    "    if clear_dummies:\n",
    "        drags = drags[drags[\"action\"] != \"drag\"]\n",
    "    \n",
    "    # Useless but avoids confusion for the reader\n",
    "    return drags\n",
    "\n",
    "def gen_drags(df):\n",
    "    drags = df[df[\"action\"].str.contains(\"drag\")].copy()\n",
    "    index = drags.index\n",
    "    drags = fix_drags(drags)\n",
    "\n",
    "    # Split starts and finishes and concatenate if not empty\n",
    "    if len(drags):\n",
    "        l = drags[::2][[\"object\", \"time\"]].reset_index(drop=True)\n",
    "        r = drags[1::2][[\"time\"]].reset_index(drop=True)\n",
    "        drags = pd.concat([l, r], axis=1)\n",
    "        drags.columns = [\"object\", \"start\", \"end\"]\n",
    "\n",
    "        # Define new columns\n",
    "        drags[\"len\"] = drags[\"end\"] - drags[\"start\"]\n",
    "        drags[\"mid\"] = (drags[\"end\"] + drags[\"start\"]) / 2\n",
    "        drags = drags[[\"object\", \"len\", \"mid\"]]\n",
    "    else:\n",
    "        drags = None\n",
    "\n",
    "    return drags, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_drags(drags):\n",
    "    obj = None\n",
    "\n",
    "    for idx, row in drags.iterrows():\n",
    "        n_act = row[\"action\"]\n",
    "        n_obj = row[\"object\"]\n",
    "\n",
    "        if \"start\" in n_act:\n",
    "            if obj is not None:\n",
    "                print(idx, \"start before end\")\n",
    "\n",
    "            obj = n_obj\n",
    "\n",
    "        elif \"end\" in n_act:\n",
    "            if obj is None:\n",
    "                print(idx, \"end without start\")\n",
    "            elif obj != n_obj:\n",
    "                print(idx, \"end of wrong obj\")\n",
    "\n",
    "            obj = None\n",
    "\n",
    "        else:\n",
    "            if obj is None:\n",
    "                print(idx, \"drag without start\")\n",
    "            elif obj != n_obj:\n",
    "                print(idx, \"drag of wrong obj\")\n",
    "\n",
    "            obj = n_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate block positions and sizes\n",
    "def process_toggles(df, duo=False):\n",
    "    # Split starts and finishes and concatenate\n",
    "    if duo:\n",
    "        l = df.iloc[:-1][[\"params\", \"time\"]]\n",
    "        r = df.iloc[1:][\"time\"].reset_index(drop=True)\n",
    "    else:\n",
    "        l = df.iloc[::2][[\"params\", \"time\"]].reset_index(drop=True)\n",
    "        r = df.iloc[1::2][\"time\"].reset_index(drop=True)\n",
    "        \n",
    "    df = pd.concat([l, r], axis=1)\n",
    "    df.columns = [\"params\", \"start\", \"end\"]\n",
    "\n",
    "    # Define block positions and sizes\n",
    "    df[\"len\"] = df[\"end\"] - df[\"start\"]\n",
    "    df[\"mid\"] = (df[\"end\"] + df[\"start\"]) / 2\n",
    "    df = df[[\"params\", \"len\", \"mid\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def gen_lasers(df):\n",
    "    laser = df[df[\"object\"] == \"laser\"].copy()\n",
    "    index = laser.index\n",
    "    laser.loc[:, \"params\"] = laser[\"params\"].apply(lambda elem : elem[\"newValue\"])\n",
    "    laser.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Do nothing if empty\n",
    "    if len(laser) and laser.iloc[-1][\"params\"]:\n",
    "        end_time = df.iloc[-1][\"time\"]\n",
    "        \n",
    "        # Add last row\n",
    "        laser.loc[len(laser)] = laser.iloc[-1]\n",
    "        laser.iloc[-1, laser.columns.get_loc(\"time\")] = end_time\n",
    "        laser.iloc[-1, laser.columns.get_loc(\"params\")] = False\n",
    "\n",
    "    # Generate block positions and sizes\n",
    "    laser = process_toggles(laser)\n",
    "    laser = laser[laser[\"params\"]].reset_index(drop=True)\n",
    "    \n",
    "    # Add label\n",
    "    laser[\"object\"] = \"laser\"\n",
    "\n",
    "    return laser, index\n",
    "\n",
    "# WIP Can be at least partially merged with gen_lasers\n",
    "# WIP Need to check if other actions happen at the same time indicating that a pdf entry was dropped\n",
    "def gen_pdfs(df):\n",
    "    pdfs = df[df[\"object\"] == \"pdf\"].copy()\n",
    "    index = pdfs.index\n",
    "    pdfs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Do nothing if empty\n",
    "    if len(pdfs) and pdfs.iloc[-1][\"params\"]:\n",
    "        end_time = df.iloc[-1][\"time\"]\n",
    "        \n",
    "        # Add last row\n",
    "        pdfs.loc[len(pdfs)] = pdfs.iloc[-1]\n",
    "        pdfs.iloc[-1, pdfs.columns.get_loc(\"time\")] = end_time\n",
    "        pdfs.iloc[-1, pdfs.columns.get_loc(\"params\")] = False\n",
    "\n",
    "    # Generate block positions and sizes\n",
    "    pdfs = process_toggles(pdfs)\n",
    "    \n",
    "    # Add label\n",
    "    pdfs[\"object\"] = \"pdf\"\n",
    "\n",
    "    return pdfs, index\n",
    "\n",
    "def gen_modes(df):\n",
    "    mode = df[df[\"object\"] == \"mode\"].copy()\n",
    "    index = mode.index\n",
    "    mode[\"params\"] = mode[\"params\"].apply(lambda elem : elem[\"value\"] == \"transmittance\")\n",
    "    mode.reset_index(drop=True, inplace=True)\n",
    "    mode.index = mode.index + 1\n",
    "\n",
    "    # Do nothing if empty\n",
    "    if len(mode):\n",
    "        end_time = df.iloc[-1][\"time\"]\n",
    "\n",
    "        # Add first row\n",
    "        mode.loc[0] = mode.loc[1]\n",
    "        mode.iloc[-1, mode.columns.get_loc(\"time\")] = 0\n",
    "        mode.iloc[-1, mode.columns.get_loc(\"params\")] = True\n",
    "\n",
    "        # Add last row\n",
    "        mode.loc[len(mode)] = mode.loc[0]\n",
    "        mode.iloc[-1, mode.columns.get_loc(\"time\")] = end_time\n",
    "        mode.iloc[-1, mode.columns.get_loc(\"params\")] = not mode.iloc[-1][\"params\"]\n",
    "\n",
    "    # Delete consecutive identical entries\n",
    "    mode.sort_index(inplace=True)\n",
    "    mode = mode[mode['params'] != mode['params'].shift(-1)]\n",
    "    mode.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Generate block positions and sizes\n",
    "    mode = process_toggles(mode, duo=True)\n",
    "    \n",
    "    # Add label\n",
    "    mode[\"object\"] = mode[\"params\"].apply(lambda elem : \"transmittance\" if elem else \"absorbance\")\n",
    "\n",
    "    return mode, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_all(df, mapping):\n",
    "    if df is None or not len(df):\n",
    "        return None, df\n",
    "    \n",
    "    n_df = pd.DataFrame()\n",
    "    ret = dict()\n",
    "\n",
    "    # Normalize time\n",
    "    n_df[\"time\"] = (df[\"time\"] - df[\"time\"][0]) / 1000\n",
    "    \n",
    "    # Extract context info\n",
    "    n_df[[\"object\", \"action\"]] = df[[\"phetioID\", \"event\"]] \\\n",
    "                                    .apply(lambda x : mapping \\\n",
    "                                        .get(x[0].split(\".\", 1)[1], dict()) \\\n",
    "                                        .get(x[1], \"other-other\"), axis=1) \\\n",
    "                                    .str.split(\"-\", 1, expand=True)\n",
    "    n_df[\"params\"] = df[\"parameters\"]\n",
    "    n_df[\"params\"] = n_df.apply(lambda elem : elem[\"params\"] if elem[\"action\"] == \"toggle\" else None, axis=1)\n",
    "    \n",
    "    # Process special categories\n",
    "    ret[\"drags\"], idx_d = gen_drags(n_df)\n",
    "    ret[\"laser\"], idx_l = gen_lasers(n_df)\n",
    "    ret[\"modes\"], idx_m = gen_modes(n_df)\n",
    "    ret[\"pdf\"],   idx_p = gen_pdfs(n_df)\n",
    "    \n",
    "    # Get the remaining entries\n",
    "    idx_o = n_df.index.difference(idx_d\n",
    "                                    .union(idx_l)\n",
    "                                    .union(idx_m)\n",
    "                                    .union(idx_p))\n",
    "    ret[\"other\"] = n_df.loc[idx_o].reset_index(drop=True)\n",
    "    \n",
    "    # Clean up the original df\n",
    "    n_df = n_df[n_df[\"action\"] != \"drag\"]\n",
    "    \n",
    "    return ret, n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_group(fig, dfs, entries, key, palette, name, h_pos, v_pos, v_shift, width, height):\n",
    "    if dfs[key] is None or not len(dfs[key]):\n",
    "        return\n",
    "    \n",
    "    cds = ColumnDataSource(dfs[key])\n",
    "    \n",
    "    for i, entry in enumerate(entries[key]):\n",
    "        fil = [GroupFilter(column_name='object', group=entry)]\n",
    "        view = CDSView(source=cds, filters=fil)\n",
    "\n",
    "        rect_kwargs = {\n",
    "            'name': name,\n",
    "            'view': view,\n",
    "            'source': cds,\n",
    "            'color': palette[entry],\n",
    "            'legend_label': entry,\n",
    "            'x': h_pos,\n",
    "            'y': v_pos + v_shift,\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "        }\n",
    "\n",
    "        fig.rect(**common_kwargs, **rect_kwargs, muted_alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_on_figure(dfs, fig, name, idx):\n",
    "    if dfs is None:\n",
    "        return\n",
    "    \n",
    "    shift = idx\n",
    "    render_group(fig, dfs, entries, \"drags\", palette[\"general\"], name, 'mid',  0,   shift, 'len', .18)\n",
    "    render_group(fig, dfs, entries, \"laser\", palette[\"toggles\"], name, 'mid',  .35, shift, 'len', .08)\n",
    "    render_group(fig, dfs, entries, \"modes\", palette[\"general\"], name, 'mid',  .25, shift, 'len', .08)\n",
    "    render_group(fig, dfs, entries, \"pdf\",   palette[\"toggles\"], name, 'mid',  .15, shift, 'len', .08)\n",
    "    render_group(fig, dfs, entries, \"other\", palette[\"general\"], name, 'time', 0,   shift, .1,    .18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_task(path, mapping, session):\n",
    "    figs, uids, s_id = [], [], 0\n",
    "    tooltips = [(\"UID\", \"$name\"), ('Component', \"@object\")]       \n",
    "    \n",
    "    for i in range(3):\n",
    "        fig = figure(**figure_kwargs, title=\"Beer's Law Lab \" + session + \" - Task \" + str(i + 1), sizing_mode='scale_width')\n",
    "        figs.append(fig)\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                uid, data = pkl.load(f)\n",
    "                uids.append(uid)\n",
    "                \n",
    "                for i in range(3):\n",
    "                    dfs, _ = gen_all(data[i], mapping)\n",
    "                    plot_on_figure(dfs, figs[i], uid, s_id)\n",
    "                \n",
    "                s_id += 1\n",
    "            except EOFError:\n",
    "                break\n",
    "       \n",
    "    for i in range(3):\n",
    "        figs[i].yaxis.major_label_overrides = dict(zip(range(len(uids)), uids))\n",
    "        figs[i].add_tools(HoverTool(tooltips=tooltips))\n",
    "        figs[i].legend.click_policy = 'hide'\n",
    "        output_file(os.path.join(\"plots\", \"bokeh\", \"by_task\", session + \" Task \" + str(i + 1) + \".html\"))\n",
    "        save(figs[i])\n",
    "        reset_output()\n",
    "\n",
    "def plot_by_student(path, mapping, session):\n",
    "    tooltips = [(\"Task\", \"$name\"), ('Component', \"@object\")]\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                uid, data = pkl.load(f)\n",
    "                fig = figure(**figure_kwargs, title=\"Beer's Law Lab \" + session + \" - UID \" + uid, sizing_mode='scale_width')\n",
    "                \n",
    "                for i in range(3):\n",
    "                    dfs, _ = gen_all(data[i], mapping)\n",
    "                    plot_on_figure(dfs, fig, str(i + 1), i)\n",
    "                    \n",
    "                fig.yaxis.major_label_overrides = dict([(j, \"Task \" + str(j + 1)) for j in range(3)])\n",
    "                fig.add_tools(HoverTool(tooltips=tooltips))\n",
    "                fig.legend.click_policy = 'hide'\n",
    "                output_file(os.path.join(\"plots\", \"bokeh\", \"by_student\", session + \" UID \" + uid + \".html\"))\n",
    "                save(fig)\n",
    "                reset_output()\n",
    "            except EOFError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False\n",
    "\n",
    "if plot:\n",
    "    for session in os.listdir(parsed_path):\n",
    "        path = os.path.join(parsed_path, session)\n",
    "        name = session.split(\".\")[0]\n",
    "        print(name)\n",
    "        plot_by_task(path, mapping, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False\n",
    "    \n",
    "if plot:\n",
    "    for session in os.listdir(parsed_path):\n",
    "        path = os.path.join(parsed_path, session)\n",
    "        name = session.split(\".\")[0]\n",
    "        print(name)\n",
    "        plot_by_student(path, mapping, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(parsed_path, \"Session 9.pkl\")\n",
    "\n",
    "dfs_list = {0 : [], 1 : [], 2 : []}\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            uid, data = pkl.load(f)\n",
    "\n",
    "            for i in range(3):\n",
    "                dfs, orig = gen_all(data[i], mapping)\n",
    "                dfs_list[i].append(dfs)\n",
    "                \n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relax(elems_orig):\n",
    "    elems = elems_orig.copy()\n",
    "    \n",
    "    for i, elem1 in enumerate(elems):\n",
    "        for elem2 in elems[i+1:]:\n",
    "            s1, e1 = elem1\n",
    "            s2, e2 = elem2\n",
    "            \n",
    "            if not ((s1 > s2 and s1 > e2) or (e1 < s2 and e1 < e2)):\n",
    "                elems.remove(elem1)\n",
    "                elems.remove(elem2)\n",
    "                new_elem = (min(s1, s2), max(e1, e2))\n",
    "                elems = elems[:i] + [new_elem] + elems[i:]\n",
    "                return elems, True\n",
    "            \n",
    "    return elems, False\n",
    "\n",
    "def usetime_old(elems):\n",
    "    if len(elems) == 0:\n",
    "        return elems\n",
    "    \n",
    "    changed = True\n",
    "    elems = sorted(elems, key=lambda e: e[0])\n",
    "    \n",
    "    while changed:\n",
    "        elems, changed = relax(elems)\n",
    "        \n",
    "    return elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_mean_std(series):\n",
    "    vcnt = len(series)\n",
    "    vsum = series.sum()\n",
    "    vmean = vsum / vcnt\n",
    "\n",
    "    vsum2 = (series ** 2).sum()\n",
    "    vstd = (vsum2 / vcnt) - vmean ** 2\n",
    "    \n",
    "    return vmean, vstd\n",
    "\n",
    "def mean_std_map(df, mids, lens=None):\n",
    "    res = dict()\n",
    "    \n",
    "    for c in df[\"object\"].unique():\n",
    "        sub_df = df[df[\"object\"] == c]\n",
    "        mid_col = sub_df[mids]\n",
    "        len_col = sub_df[lens] if lens else 0.5\n",
    "        \n",
    "        mean_up = (mid_col * len_col).sum()\n",
    "        mean2_up = ((mid_col ** 2) * len_col).sum()\n",
    "        mean_down = len_col.sum() if lens else 0.5 * len(mid_col)\n",
    "        \n",
    "        loc_dict = {\n",
    "            \"mean_up\": mean_up,\n",
    "            \"mean2_up\": mean2_up,\n",
    "            \"mean_down\": mean_down\n",
    "        }\n",
    "        \n",
    "        mean_down = len(mid_col)    \n",
    "        \n",
    "        if lens:\n",
    "            mean_up = len_col.sum()\n",
    "            mean2_up = (len_col ** 2).sum()\n",
    "        else:\n",
    "            mean_up = 0.5 * mean_down\n",
    "            mean2_up = 0.25 * mean_down\n",
    "        \n",
    "        len_dict = {\n",
    "            \"mean_up\": mean_up,\n",
    "            \"mean2_up\": mean2_up,\n",
    "            \"mean_down\": mean_down\n",
    "        }\n",
    "        \n",
    "        res[c] = {\n",
    "            \"loc\": loc_dict,\n",
    "            \"len\": len_dict\n",
    "        }\n",
    "    \n",
    "    return res\n",
    "\n",
    "def mean_std_reduce(elems):\n",
    "    temp = dict()\n",
    "    res = dict()\n",
    "\n",
    "    for elem in elems:\n",
    "        for k in elem.keys():\n",
    "            if temp.get(k) is None:\n",
    "                temp[k] = elem[k].copy()\n",
    "            else:\n",
    "                for k2 in elem[k].keys():\n",
    "                    for k3 in elem[k][k2]:\n",
    "                        temp[k][k2][k3] += elem[k][k2][k3]\n",
    "\n",
    "    for k, v in temp.items():\n",
    "        sub = dict()\n",
    "\n",
    "        for k2, v2 in v.items():\n",
    "            if abs(v2[\"mean_down\"]) < 0.000001:\n",
    "                mean = 0\n",
    "                mean2 = 0\n",
    "            else:\n",
    "                mean = v2[\"mean_up\"] / v2[\"mean_down\"]\n",
    "                mean2 = v2[\"mean2_up\"] / v2[\"mean_down\"]\n",
    "\n",
    "            # We use max to correct for floating point inaccuracies that would cause a negative variance in certain cases\n",
    "            std = max(0, mean2 - (mean ** 2)) ** .5\n",
    "\n",
    "            sub[k2] = {\n",
    "                \"mean\": mean,\n",
    "                \"std\": std\n",
    "            }\n",
    "\n",
    "        res[k] = sub   \n",
    "    \n",
    "    reform = {(innerKey, outerKey): values for outerKey, innerDict in res.items() for innerKey, values in innerDict.items()}\n",
    "    reform = pd.DataFrame(reform)\n",
    "    \n",
    "    loc_df = reform[\"loc\"].T\n",
    "    len_df = reform[\"len\"].T\n",
    "    \n",
    "    return loc_df, len_df\n",
    "\n",
    "def usetime(elems):\n",
    "    new_elems = []\n",
    "    mem = elems[0]\n",
    "    \n",
    "    for elem in elems[1:]:\n",
    "        s1, e1 = mem\n",
    "        s2, e2 = elem\n",
    "        \n",
    "        if ((s1 > s2 and s1 > e2) or (e1 < s2 and e1 < e2)):\n",
    "            new_elems.append(mem)\n",
    "            mem = elem\n",
    "        else:\n",
    "            mem = (min(s1, s2), max(e1, e2))\n",
    "            \n",
    "    new_elems.append(mem)\n",
    "            \n",
    "    return new_elems\n",
    "\n",
    "def first_uses(m, l, n):\n",
    "    if len(m) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    gb = pd.concat([n, m - l], axis=1).groupby(\"object\")\n",
    "    gb = pd.concat([v.iloc[0:1] for k, v in gb])\n",
    "    gb.columns = [\"object\", 0]\n",
    "    \n",
    "    return gb\n",
    "\n",
    "def tc_drags(drags):\n",
    "    m = drags[\"mid\"]\n",
    "    l = drags[\"len\"] / 2\n",
    "    n = drags[\"object\"]\n",
    "    \n",
    "    active = pd.concat([m-l, m+l], axis=1)\n",
    "    active = active.itertuples(index=False, name=None)\n",
    "    \n",
    "    firsts = first_uses(m, l, n)\n",
    "    \n",
    "    times = pd.Series(dict([(k, v[\"len\"].sum()) for k, v in drags.groupby(\"object\")]))\n",
    "\n",
    "    return list(active), firsts, times\n",
    "\n",
    "def tc_clicks(clicks, tpc):\n",
    "    m = clicks[\"time\"]\n",
    "    n = clicks[\"object\"]\n",
    "    \n",
    "    active = pd.concat([m, m+tpc], axis=1)\n",
    "    active = active.itertuples(index=False, name=None)\n",
    "    \n",
    "    firsts = first_uses(m, 0, n)\n",
    "    \n",
    "    times = pd.DataFrame([(k, len(v) * tpc) for k, v in clicks.groupby(\"object\")]).set_index(0)[1]\n",
    "    times.index.name = None\n",
    "    \n",
    "    return list(active), firsts, times\n",
    "\n",
    "def tc_rest(rest, tpc):\n",
    "    m = rest[\"mid\"]\n",
    "    l = rest[\"len\"] / 2\n",
    "    n = rest[\"object\"]\n",
    "\n",
    "    temp = pd.concat([m-l, m+l], axis=1)\n",
    "    starts = temp.iloc[:, 0]\n",
    "    ends = temp.iloc[:, 1]\n",
    "\n",
    "    starts = pd.concat([starts-tpc, starts+tpc], axis=1)\n",
    "    ends = pd.concat([ends-tpc, ends+tpc], axis=1)\n",
    "    ends.columns = [0, 0]\n",
    "    active = pd.concat([starts, ends], axis=0, ignore_index=True).itertuples(index=False, name=None)\n",
    "        \n",
    "    firsts = first_uses(m, 0, n)\n",
    "    \n",
    "    if len(rest) == 0:\n",
    "        times = pd.Series(dtype=np.float64)\n",
    "    else:\n",
    "        times = pd.DataFrame([(k, len(v) * 2 * tpc) for k, v in rest.groupby(\"object\")]).set_index(0)[1]\n",
    "        times.index.name = None\n",
    "    \n",
    "    return list(active), firsts, times\n",
    "\n",
    "def time_cover(dfs, tpc):\n",
    "    active = []\n",
    "    firsts = []\n",
    "    times = []\n",
    "    means_stds = []\n",
    "    \n",
    "    dfs = {k: df for k, df in dfs.items() if df is not None and len(df) > 0}\n",
    "    \n",
    "    for k, df in dfs.items():\n",
    "        if k == \"drags\" or k == \"pdf\":\n",
    "            a, f, t = tc_drags(df)\n",
    "        elif k == \"other\":\n",
    "            a, f, t = tc_clicks(df, tpc)\n",
    "        elif k == \"modes\":\n",
    "            a, f, t = tc_rest(df.loc[1:], tpc)\n",
    "        else:\n",
    "            a, f, t = tc_rest(df, tpc)\n",
    "            \n",
    "        active += a\n",
    "        firsts.append(f)\n",
    "        times.append(t)\n",
    "            \n",
    "        if k == \"other\":\n",
    "            ms = mean_std_map(df, \"time\")\n",
    "        else:\n",
    "            ms = mean_std_map(df, \"mid\", \"len\")\n",
    "            \n",
    "        means_stds.append(ms)\n",
    "        \n",
    "    active = usetime(active)\n",
    "    active = sorted(active, key=lambda a: a[0])\n",
    "    \n",
    "    breaks = pd.DataFrame(active)\n",
    "    breaks[0] = breaks[0][1:].reset_index(drop=True)\n",
    "    breaks = breaks.iloc[:-1]\n",
    "    \n",
    "    ms_breaks = breaks.copy()\n",
    "    ms_breaks[\"mid\"] = (ms_breaks[1] + ms_breaks[0]) / 2\n",
    "    ms_breaks[\"len\"] = ms_breaks[0] - ms_breaks[1]\n",
    "    ms_breaks[\"object\"] = \"breaks\"\n",
    "    ms_breaks = ms_breaks.drop([0, 1], axis=1)\n",
    "    \n",
    "    breaks = (breaks[0] - breaks[1]).apply(lambda x: 0 if x < 0 else x)\n",
    "    \n",
    "    ms_all = pd.DataFrame(active)\n",
    "    ms_all[\"mid\"] = (ms_all[1] + ms_all[0]) / 2\n",
    "    ms_all[\"len\"] = ms_all[1] - ms_all[0]\n",
    "    ms_all[\"object\"] = \"all\"\n",
    "    ms_all = ms_all.drop([0, 1], axis=1)\n",
    "    \n",
    "    means_stds.append(mean_std_map(ms_breaks, \"mid\", \"len\"))\n",
    "    means_stds.append(mean_std_map(ms_all, \"mid\", \"len\"))\n",
    "    loc_means_stds, len_means_stds = mean_std_reduce(means_stds)\n",
    "    \n",
    "    firsts = pd.concat(firsts)\n",
    "    firsts = firsts.sort_values(0).reset_index(drop=True)\n",
    "    firsts = firsts.drop_duplicates(subset=[\"object\"])\n",
    "    firsts = firsts.set_index(\"object\")[0]\n",
    "    firsts.index.name = None\n",
    "    \n",
    "    counts = pd.concat(dfs[k][\"object\"].value_counts() for k in dfs.keys())\n",
    "    counts = counts.groupby(level=0).sum()\n",
    "    counts[\"breaks\"] = len(breaks)\n",
    "    \n",
    "    adder = partial(pd.Series.add, fill_value=0)\n",
    "    times = reduce(adder, times)\n",
    "    times[\"breaks\"] = sum(breaks)\n",
    "            \n",
    "    return {\n",
    "        \"firsts\" : firsts,\n",
    "        \"counts\" : counts,\n",
    "        \"times\" : times,\n",
    "        \"loc_means_stds\": loc_means_stds,\n",
    "        \"elem_means_stds\": len_means_stds\n",
    "    }\n",
    "    \n",
    "time_per_click = 0.5\n",
    "a = time_cover(dfs, time_per_click)\n",
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dfs(dfs):\n",
    "    res = []\n",
    "    \n",
    "    for k, df in dfs.items():\n",
    "        if k == \"other\":\n",
    "            x = df.copy()\n",
    "            x[\"mid\"] = x[\"time\"]\n",
    "            x[\"len\"] = 0.5\n",
    "            x = x.drop([\"action\", \"params\", \"time\"], axis=1)\n",
    "            \n",
    "            res.append(x)\n",
    "        else:\n",
    "            res.append(df)\n",
    "            \n",
    "    res = pd.concat(res, axis=0) \\\n",
    "            .drop([\"params\"], axis=1) \\\n",
    "            .sort_values(\"mid\", axis=0) \\\n",
    "            .dropna(how=\"any\", axis=0) \\\n",
    "            .reset_index(drop=True)\n",
    "    \n",
    "    obj = set(res[\"object\"].unique())\n",
    "    \n",
    "    return res, obj\n",
    "\n",
    "#with open(\"jade_test.pkl\", \"wb\") as f:\n",
    "#    pkl.dump(fix_dfs(dfs), f)\n",
    "    \n",
    "#fix_dfs(dfs).to_csv(\"jade_test.csv\")\n",
    "fix_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_merge(df0, df1):\n",
    "    \n",
    "def df_variants(df0, df1):\n",
    "    df0[\"extra\"] = 0\n",
    "    df1[\"extra\"] = 1\n",
    "    \n",
    "    comb = pd.concat([df0, df1], axis=0).reset_index(drop=True)\n",
    "    comb_str = comb.copy()\n",
    "    comb_str[\"object\"] = comb_str[\"object\"] + comb_str[\"extra\"].astype(str)\n",
    "    comb_str = comb_str.drop(\"extra\", axis=1)\n",
    "    \n",
    "    return comb, comb_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = fix_dfs(dfs)\n",
    "c, d = fix_dfs(dfs)\n",
    "e, f = df_variants(a, c)\n",
    "pd.concat([None, f], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obj = set()\n",
    "\n",
    "for session in os.listdir(parsed_path):\n",
    "    print(session)\n",
    "    with open(os.path.join(parsed_path, session), \"rb\") as f_in:\n",
    "        j = 0\n",
    "        try:\n",
    "            while True:\n",
    "                try:\n",
    "                    uid, data = pkl.load(f_in)\n",
    "                    print(\"\\t\" + str(j) + \" - \" + uid, end=\"\\r\")\n",
    "                    \n",
    "                    curr = []\n",
    "\n",
    "                    for i in range(3):\n",
    "                        dfs, _ = gen_all(data[i], mapping)\n",
    "\n",
    "                        if dfs is not None and i < 2:\n",
    "                            df, obj = fix_dfs(dfs)\n",
    "                            df[\"extra\"] = i\n",
    "                            all_obj |= obj\n",
    "                            curr.append(df)\n",
    "                            #df.to_csv(os.path.join(\"csv\", uid + \".csv\"))\n",
    "                            \n",
    "                    if len(curr) == 2:\n",
    "                        comb = pd.concat(curr, axis=0).reset_index(drop=True)\n",
    "                        comb_str = comb.copy()\n",
    "                        comb_str[\"object\"] = comb_str[\"object\"] + comb_str[\"extra\"].astype(str)\n",
    "                        comb_str = comb_str.drop(\"extra\", axis=1)\n",
    "                        \n",
    "                        comb.to_csv(os.path.join(\"csv_extra\", uid + \".csv\"))\n",
    "                        comb_str.to_csv(os.path.join(\"csv_merge\", uid + \".csv\"))\n",
    "                        \n",
    "                except EOFError:\n",
    "                    break\n",
    "                j += 1\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted!\")\n",
    "                \n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obj = set()\n",
    "\n",
    "for session in os.listdir(parsed_path):\n",
    "    print(session)\n",
    "    with open(os.path.join(parsed_path, session), \"rb\") as f_in:\n",
    "        j = 0\n",
    "        try:\n",
    "            while True:\n",
    "                try:\n",
    "                    uid, data = pkl.load(f_in)\n",
    "                    print(\"\\t\" + str(j) + \" - \" + uid, end=\"\\r\")\n",
    "\n",
    "                    for i in range(3):\n",
    "                        dfs, _ = gen_all(data[i], mapping)\n",
    "\n",
    "                        if dfs is not None and i == 1:\n",
    "                            #curr_data = time_cover(dfs, time_per_click)\n",
    "                            #pkl.dump((uid, i, curr_data), f_out)\n",
    "                            df, obj = fix_dfs(dfs)\n",
    "                            all_obj |= obj\n",
    "                            df.to_csv(os.path.join(\"csv\", uid + \".csv\"))\n",
    "                except EOFError:\n",
    "                    break\n",
    "                j += 1\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted!\")\n",
    "                \n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(all_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"objects.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(list(all_obj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION SHOULD BE TREATED AS A DRAG SINCE WE OPEN AND CLOSE THE LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "Might wanna check if all message indices are here in the future\n",
    "\n",
    "Make modules work `from helpers import parsing as par`\n",
    "\n",
    "Tester toutes les entrées PHET qui existent dans les logs\n",
    "\n",
    "Report on the number of wrong drag entries\n",
    "\n",
    "Verifier le parsing du laser\n",
    "\n",
    "Mean and standard deviation of each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next week goal is to have plots that show us for each feature how they are distributed between the different students in order to choose the RF features\n",
    "\n",
    "Demo de défense avec labo possible\n",
    "\n",
    "Lire fonctions depuis des helper files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTRACTION OF TIME RELATED DATA\n",
    "\n",
    "with open(os.path.join(data_path, \"time_data.pkl\"), \"wb\") as f_out:\n",
    "    for session in os.listdir(parsed_path):\n",
    "        print(session)\n",
    "        with open(os.path.join(parsed_path, session), \"rb\") as f_in:\n",
    "            j = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    try:\n",
    "                        uid, data = pkl.load(f_in)\n",
    "                        print(\"\\t\" + str(j) + \" - \" + uid, end=\"\\r\")\n",
    "                        \n",
    "                        for i in range(3):\n",
    "                            dfs, _ = gen_all(data[i], mapping)\n",
    "\n",
    "                            if dfs is not None:\n",
    "                                curr_data = time_cover(dfs, time_per_click)\n",
    "                                pkl.dump((uid, i, curr_data), f_out)\n",
    "                    except EOFError:\n",
    "                        break\n",
    "                    j += 1\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Interrupted!\")\n",
    "                \n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm-env",
   "language": "python",
   "name": "pdm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
